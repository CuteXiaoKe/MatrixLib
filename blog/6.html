<p>One of the central challenges throughout the development of MatrixLib has been finding a way of computing eigenvalues and eigenvectors of a matrix. This is both frustrating and somewhat unsurprising, as the eigensystem of a matrix turns out to be a panacea for quick and stable computation of inverse matrices and of determinants. In particular, if the eigenvalues of a matrix $$A$$ are distinct $$\lambda_1,\lambda_2,\dots$$, then we can compute the similar matrix $$A$$ in its eigenbasis, $$A=Q\Lambda Q^{-1}$$, where $$Q$$ is built on the eigenvectors of $$A$$, and $$\Lambda=\mathrm{diag}(\lambda_1,\lambda_2,\dots)$$. If we have this information, we get nearly for free</p>
<p>\[\det(A)=\det(Q)\det(\Lambda)\det(Q^{-1})=\prod_{i=1}^{n} \lambda_i\]</p>
<p>as well as $$A^{-1} = Q^{-1}\Lambda Q$$, where $$[\Lambda^{-1}]_{ij}=\lambda_{ij}^{-1}$$.</p>
<p>Computing eigenvalues by hand in simple cases is essentially simpler than doing it by computer. It is well known that the eigenvalues of a matrix $$A$$ are the roots of the characteristic polynomial of $$A$$, i.e. the numbers $$\lambda$$ that satisfy $$\det(A-\lambda I)=0$$. However easy it is to apply this property by hand, though, it is a mess if done by computer. Root-finding algorithms are in many cases unstable, slow, and poorly understood. (In fact, a typical method for finding the roots of a polynomial is to build the companion matrix and compute its eigenvalues).</p>
<p>In an ironic twist, the existence of this method for computing eigenvalues actually worsens the situation significantly. The <a href="http://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem">Abel-Ruffini theorem</a> of modern algebra tells us that the Galois group for general polynomials of degree higher than four is not solvable, meaning that fifth degree polynomials and up do not have solutions expressible by radicals; that is, we cannot write down a general solution for the eigenvalues of a matrix bigger than $$4\times 4$$. Moreover - and this is the worse part - any algorithm that computes the eigenvalues of such a matrix in a finite number of steps implies a closed-form solution to the characteristic polynomial, and therefore cannot exist.</p>
<p>So to compute the eigenvalues of a general sufficiently large matrix, we need to implement an algorithm that cannot complete in a finite number of steps, predict whether it will converge to a finite solution, and then project what that finite solution will be. The most widely studied and used method for doing this is the QR algorithm, which relies on computing a QR factorization, writing a matrix $$A=QR$$, where $$Q$$ is a unitary matrix and $$R$$ is an upper triangular matrix.</p>
<p>In each iteration of the QR algorithm, we compute $$Q_kR_k=A_{k-1}$$, and then $$A_k=R_kQ_k$$. Observe that</p>
<p>\[A_k=R_kQ_k=Q_k^{-1}Q_kR_kQ_k=Q_k^{-1}A_{k-1}Q_k,\]</p>
<p>so that this transformation is a similarity transform, which preserves the eigenvalues of the matrix. This is easy to see by noting that similar matrices have the same characteristic polynomial. We iterate this process until the sequence $$\{A_k\}$$ converges to either a matrix whose bottom row is all zeros except for the last entry, or whose bottom two rows are all zeros except for the bottom right $$2\times 2$$ submatrix. In the first case, the bottom right element is an eigenvalue, and we reiterate on the upper left submatrix. In the second case, we compute the eigenvalues of the bottom right block matrix; one can evaluate the characteristic polynomial on such a matrix and easily derive that the eigenvalues of a $$2\times 2$$ matrix $$A$$ are</p>
<p>\[\lambda=\frac{1}{2}\left(\mathrm{tr}(A)\pm\sqrt{\mathrm{tr}^2(A)-4\det(A)}\right).\]</p>
<p>For real matrices with real eigenvalues, this works. However, it is a slow method, showing a possibility of converging only for small epsilon and giving results with about three digits of precision. But we can modify this algorithm by introducing a 'shift.' The convergence rate of the QR algorithm is based on the ratio of eigenvalues of the matrix $$|\lambda_1/\lambda_2|$$. By shifting the matrix by a number $$\mu$$ that is much closer to $$\lambda_2$$ than to $$\lambda_1$$, the convergence rate becomes $$|(\lambda_1-\mu)/(\lambda_2-\mu)|$$, which can be much faster. Then in our iteration, we instead compute</p>
<p>\[Q_{k}R_{k}=A_{k}-\mu I, A_{k+1}=R_{k}Q_{k}+\mu I.\]</p>
<p>For the time being, I've implemented a naive shifting algorithm, which just uses the bottom right element of the current matrix for $$\mu$$. However, even this has drastically affected performance. The routine now completes for a $$3\times 3$$ matrix generally in under 10 ms, gives six digits of accuracy, and works for any epsilon down to the precision of a double.</p>
<p>This is the current state of my eigensolver. My next goals are to write a routine for computing the corresponding eigenvectors, implement the smarter shifting method, try to figure out how to increase the accuracy of my results, and attempt to extend the current routine so that it can find complex eigenvalues of real matrices as well as eigenvalues of complex matrices.</p>