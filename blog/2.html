<p>One of the biggest problems in developing MatrixLib has been that of numerical stability. In many computations there are roundoff errors that accumulate over time, yielding results such as .999999999998 instead of 1 or $$ 10^{-14}$$ instead of 0. While these errors present no problem to a human reading the results - provided that the numbers they are working with are indeed large enough to be distinguished from roundoff error - this does present a challenging problem for the computer to overcome. For example, there were problems with Gauss-Jordan elimination because the computer would read a column that had $$10^{-14}$$ as an entry when it was supposed to be zero and count it as a pivot.</p>
<p>One way I have partly overcome these problems was to abridge the equals() method in the ComplexNumber class so that instead of directly comparing the real and imaginary parts of the number, it checks whether their difference is less than some number epsilon, which currently by default is set to $$ 10^{-15}$$.</p>
<p>This paradigm has the further advantage that the epsilon, which is a static field of ComplexNumber, can be set by the user. This way, if the user is working with particularly small numbers, they can set the epsilon to be relatively small, thus avoiding the problem of the library interpreting actual data as zero.</p>
<p>Other methods have been revamped in the interest of numerical stability. Gauss-Jordan elimination has been re-implemented with partial pivoting, which rotates the largest-pivoted row to the top of the matrix at every iteration. This helps to avoid subtractions of rows that are numerically close to one another, reducing the amount of error due to roundoff. While not perfect, this seems to be the best existing solution. Gram-Schmidt has also been modified in the traditional way, so that each new orthogonal vector is computed before the start of the next iteration, so that they are each orthogonalized against the error introduced in the previous iteration.</p>
<p>There are other problems with numerical stability still, in particular in computing matrix inverses via Gauss-Jordan and orthonormalizations via Gram-Schmidt. The current matrix inverse routine generates results with particularly large errors. I am currently looking into alternative methods of computing matrix inverses that are more numerically stable, such as using a pivoted Cholesky decomposition. And while the modified Gram-Schmidt process is somewhat stable, other algorithms for orthogonalizing a set of vectors that use Householder transformations or Givens rotations are known to be more stable, and I am looking into implementing one of those as well for the orthonormalize() routine.</p>