<p>Computing the eigenvectors corresponding to the eigenvalues of a matrix turns out to be roughly as difficult as computing the eigenvalues themselves. Nevertheless, finding eigenvectors is important for computing the Schur factorization and - in the future - the singular value decomposition.</p>
<p>I surveyed a number of methods for computing the eigenvectors of a matrix. The most simple of these I found was the <a href="http://en.wikipedia.org/wiki/Power_iteration">power iteration</a>,  which has the drawback that it only computes the dominant eigenvalue and associated eigenvector.</p>
<p>A modification of the power iteration that is applicable to every eigenvalue of a matrix is the inverse iteration, which is what MatrixLib currently implements. For an approximation $$\lambda$$ to an eigenvalue of a matrix $$A$$, we first compute the operator $$X=(A-\lambda I)^{-1}$$. Note that it is crucial that $$\lambda$$ be an approximation to an eigenvalue, because if $$\lambda$$ is an eigenvalue of $$A$$ then $$A-\lambda I$$ will be singular. MatrixLib gets around this problem with a technique that comes off rather like a hack: it repeatedly multiplies $$\lambda$$ by .999 until it is far enough from the actual eigenvalue that $$A-\lambda I$$ is nonsingular.</p>
<p>We pick an initial guess $$b_0$$ with magnitude 1. Then, at the $$k$$th iteration of the algorithm, we compute the new vector</p>
<p>\[b_{k+1} = \frac{1}{||Xb_k||} Xb_k.\]</p>
<p>Eventually, the sequence of vectors $$\{b_i\}$$ will converge to an eigenvector of the matrix (except for a set of initial guesses which has measure 0).</p>
<p>A current problem in MatrixLib - perhaps the current outstanding problem -  is that this routine does not converge on the right vector for some matrices. I have not yet determined for which matrices this occurs, nor why. Fortunately, computing eigenvectors does not play a role in the implementation of other algorithms, the major exception being computing the Schur decomposition, so the current damage is minimized.</p>