<p>Computing matrix inverses in a numerical stable fashion has proven to be a surprising challenge. The standard algorithm for doing this is to write down the augmented matrix $$[A | I]$$ and then apply Gauss-Jordan reduction to give the matrix $$[I | A^{-1}]$$. This process however is not numerically stable, and results in an inverse matrix with imprecise entries, even for an easy example from the special linear group such as $$\begin{pmatrix}5&19\\1&4\end{pmatrix}$$.</p>
<p>On the other hand, computing the matrix inverse of a lower triangular matrix is easy and numerically stable. It is as simple as reading off the solution to the bottom row, then back-substituting upwards until the entire matrix has been inverted. Similarly, a right triangular matrix can be transposed and then had this process applied to it.</p>
<p>Moreover, in the special case that the matrix is Hermitian and positive definite, a pivoted Cholesky decomposition can be applied to write the matrix $$A=LL*$$ in terms of a lower triangular matrix and its conjugate transpose. Then</p>
<p>\[A^{-1} = (LL*)^{-1} = L^{-*}L^{-1},\]</p>
<p>where the inverses of the lower triangular matrices can be computed easily as described above.</p>
<p>The current MatrixLib implementation checks to see whether the given matrix satisfies any of these characteristics and applies them if it does. Otherwise, it defaults to the general Gauss-Jordan reduction algorithm. Based on my research, it seems that this is the optimal way of computing a matrix inverse. It is further advised that due to these deficiencies, a linear system is solved with e.g. LU factorization rather than explicitly computing the matrix inverse. In particular, to solve $$Ax=b$$, we write $$A=LU$$ so that $$LUx=b$$, or $$x=U^{-1}L^{-1}b$$, where these inverses can be computed easily as described above.</p>